{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Luca Colombo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import pos_tag, bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    #function to read the data\n",
    "    #the chosen encoding has proven to be optimal given the input data\n",
    "    with open(path, 'rb',) as file:\n",
    "        data = file.read().decode('utf8', 'surrogateescape')\n",
    "        data = data.replace('\\n', '')\n",
    "    return data\n",
    "\n",
    "\n",
    "def senti_reader(sentiment, stemmer = False):\n",
    "    #this function takes the sentiment as an argument and returns a list\n",
    "    #with all train and test observations for the specific sentiment\n",
    "    #the function relies on positive and negative reviews data being located\n",
    "    #in separate folders\n",
    "    out = []\n",
    "    folder = os.path.join('review_polarity.v2/txt_sentoken/', sentiment)\n",
    "    files = [os.path.join(folder, x) for x in sorted(os.listdir(folder)) if x[-4:len(x)] == '.txt']\n",
    "    for path in files:\n",
    "        data = read_files(path)\n",
    "        #remove extra spaces, then tokenize\n",
    "        review = word_tokenize(data.strip())\n",
    "        #remove punctuation\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        clean_review = [regex.sub('', word) for word in review]\n",
    "        #remove empty strings\n",
    "        clean_review = list(filter(None, clean_review))\n",
    "        if stemmer:\n",
    "            stemmer = PorterStemmer()\n",
    "            stemmed_review = [stemmer.stem(token) for token in clean_review]\n",
    "            out.append(stemmed_review)\n",
    "        else:\n",
    "            out.append(clean_review)\n",
    "    return out\n",
    "\n",
    "\n",
    "def show_most_informative_features(vectorizer, classifier, n=10):\n",
    "    #this function prints the top n most informative features for each class\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()  \n",
    "    topn_pos_class = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_neg_class = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]    \n",
    "\n",
    "    print(\"Important words in positive reviews\")\n",
    "    for coef, feature in topn_pos_class:\n",
    "        print(class_labels[1], coef, feature) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in negative reviews\")\n",
    "    for coef, feature in topn_neg_class:\n",
    "        print(class_labels[0], coef, feature)\n",
    "\n",
    "\n",
    "def filter_adj_adv(text):\n",
    "    #this function, when given a single review, returns only words that\n",
    "    #are either adjectives or adverbs\n",
    "    tags = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "    pos_tagged = pos_tag(text)\n",
    "    result = [x[0] for x in pos_tagged if x[1] in tags]\n",
    "    return(result)\n",
    "\n",
    "\n",
    "def pos_neg_word_ratio(review, positive_words, negative_words):\n",
    "    #this function returns the ratio of positive words over negative words\n",
    "    #in a given review\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for word in review:\n",
    "        if word in positive_words:\n",
    "            pos += 1\n",
    "        if word in negative_words:\n",
    "            neg += 1\n",
    "    if neg != 0:\n",
    "        ratio = pos/neg\n",
    "    else:\n",
    "        ratio = pos\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in memory all positive and negative reviews\n",
    "pos = senti_reader('pos')\n",
    "neg = senti_reader('neg')\n",
    "\n",
    "#perform train-test split\n",
    "#notice that we have sorted reviews in list by filename\n",
    "#so we can take first 900 as train and last 100 as test and we will\n",
    "#have the required split ('cv9XX' reviews are in test)\n",
    "train_pos = pos[0:900]\n",
    "test_pos = pos[900:1000]\n",
    "train_neg = neg[0:900]\n",
    "test_neg = neg[900:1000]\n",
    "\n",
    "#make train and test set, combining positive and negative reviews\n",
    "train = train_pos + train_neg\n",
    "test = test_pos + test_neg\n",
    "#make labels for train and test\n",
    "train_labels = [1] * 900 + [0] * 900\n",
    "test_labels = [1] * 100 + [0] * 100\n",
    "\n",
    "#small sanity check: want as many observations as labels\n",
    "len(test_labels) == len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 86.0%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 900.0 the\n",
      "1 900.0 of\n",
      "1 899.0 to\n",
      "1 899.0 is\n",
      "1 899.0 and\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 899.0 the\n",
      "0 899.0 of\n",
      "0 899.0 and\n",
      "0 898.0 to\n",
      "0 898.0 is\n"
     ]
    }
   ],
   "source": [
    "#get word counts\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 82.5%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 37055.0 the\n",
      "1 17716.0 and\n",
      "1 16623.0 of\n",
      "1 14733.0 to\n",
      "1 12891.0 is\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 31363.0 the\n",
      "0 13981.0 and\n",
      "0 13798.0 of\n",
      "0 13784.0 to\n",
      "0 10408.0 is\n"
     ]
    }
   ],
   "source": [
    "#get word counts\n",
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 85.0%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 2643.0 not\n",
      "1 1660.0 nt\n",
      "1 1632.0 more\n",
      "1 1289.0 so\n",
      "1 1247.0 most\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 2416.0 not\n",
      "0 2128.0 nt\n",
      "0 1482.0 so\n",
      "0 1391.0 just\n",
      "0 1361.0 more\n"
     ]
    }
   ],
   "source": [
    "#only keep adjectives and adverbs\n",
    "train_adj_adv = [filter_adj_adv(review) for review in train]\n",
    "test_adj_adv = [filter_adj_adv(review) for review in test]\n",
    "\n",
    "#get word counts\n",
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train_adj_adv])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test_adj_adv])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 86.5%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 22.492825932912133 movie\n",
      "1 18.509989709201456 like\n",
      "1 17.841205636352782 does\n",
      "1 16.896498279193935 story\n",
      "1 16.830390064481563 just\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 27.110438636527537 movie\n",
      "0 21.182340191886198 like\n",
      "0 19.920244154434403 just\n",
      "0 18.74488216623521 bad\n",
      "0 17.754498252923117 does\n"
     ]
    }
   ],
   "source": [
    "#get word counts\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, stop_words='english', sublinear_tf=True)\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 84.0%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 845.0 of the\n",
      "1 786.0 in the\n",
      "1 682.0 the film\n",
      "1 657.0 to the\n",
      "1 635.0 and the\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 828.0 of the\n",
      "0 796.0 in the\n",
      "0 640.0 the film\n",
      "0 619.0 to be\n",
      "0 578.0 to the\n"
     ]
    }
   ],
   "source": [
    "#get word counts\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(2,2))\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4 is the best performing model, model 2 is the worst performing. When looking at the 5 most important words in positive and negative reviews we notice that all models have a substantial overlap and that most of the words we see here are stop words.   \n",
    "Moving away from the purely numerical evaluation of models based on accuracy, we see that no model is giving a strong and reliable series of the most important words in positive and negative reviews and all the models appear to not be performing well from this point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in memory all positive and negative reviews, this time use Porter's stemmer\n",
    "pos_stem = senti_reader('pos', stemmer = True)\n",
    "neg_stem = senti_reader('neg', stemmer = True)\n",
    "\n",
    "#perform train-test split\n",
    "#notice that we have sorted reviews in list by filename\n",
    "#so we can take first 900 as train and last 100 as test and we will\n",
    "#have the required split ('cv9XX' reviews are in test)\n",
    "train_pos_stem = pos_stem[0:900]\n",
    "test_pos_stem = pos_stem[900:1000]\n",
    "train_neg_stem = neg_stem[0:900]\n",
    "test_neg_stem = neg_stem[900:1000]\n",
    "\n",
    "#make train and test set, combining positive and negative reviews\n",
    "train_stem = train_pos_stem + train_neg_stem\n",
    "test_stem = test_pos_stem + test_neg_stem\n",
    "#make labels for train and test\n",
    "train_labels_stem = [1] * 900 + [0] * 900\n",
    "test_labels_stem = [1] * 100 + [0] * 100\n",
    "\n",
    "#small sanity check: want as many observations as labels\n",
    "len(test_labels_stem) == len(test_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 85.5%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 900.0 the\n",
      "1 900.0 of\n",
      "1 899.0 to\n",
      "1 899.0 is\n",
      "1 899.0 and\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 899.0 the\n",
      "0 899.0 of\n",
      "0 899.0 and\n",
      "0 898.0 to\n",
      "0 898.0 is\n"
     ]
    }
   ],
   "source": [
    "#get word counts\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "train_features_stem = vectorizer.fit_transform([' '.join(review) for review in train_stem])\n",
    "test_features_stem = vectorizer.transform([' '.join(review) for review in test_stem])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features_stem, train_labels_stem)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features_stem)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels_stem, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 83.5%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 37055.0 the\n",
      "1 17716.0 and\n",
      "1 16627.0 of\n",
      "1 14733.0 to\n",
      "1 12891.0 is\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 31363.0 the\n",
      "0 13981.0 and\n",
      "0 13799.0 of\n",
      "0 13784.0 to\n",
      "0 10408.0 is\n"
     ]
    }
   ],
   "source": [
    "#get word counts\n",
    "vectorizer = CountVectorizer()\n",
    "train_features_stem = vectorizer.fit_transform([' '.join(review) for review in train_stem])\n",
    "test_features_stem = vectorizer.transform([' '.join(review) for review in test_stem])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features_stem, train_labels_stem)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features_stem)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels_stem, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 82.5%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 2656.0 not\n",
      "1 1880.0 hi\n",
      "1 1633.0 more\n",
      "1 1590.0 nt\n",
      "1 1269.0 so\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 2420.0 not\n",
      "0 1976.0 nt\n",
      "0 1449.0 so\n",
      "0 1391.0 just\n",
      "0 1368.0 hi\n"
     ]
    }
   ],
   "source": [
    "#only keep adjectives and adverbs\n",
    "train_adj_adv_stem = [filter_adj_adv(review) for review in train_stem]\n",
    "test_adj_adv_stem = [filter_adj_adv(review) for review in test_stem]\n",
    "\n",
    "#get word counts\n",
    "vectorizer = CountVectorizer()\n",
    "train_features_stem = vectorizer.fit_transform([' '.join(review) for review in train_adj_adv_stem])\n",
    "test_features_stem = vectorizer.transform([' '.join(review) for review in test_adj_adv_stem])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features_stem, train_labels_stem)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features_stem)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels_stem, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 84.5%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 23.34833590332707 wa\n",
      "1 21.172466772419387 charact\n",
      "1 20.7167222993606 like\n",
      "1 19.75065962931403 make\n",
      "1 19.33995912700573 time\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 25.890721206769395 wa\n",
      "0 23.61681537014885 like\n",
      "0 21.61590003839188 charact\n",
      "0 21.10268885729731 just\n",
      "0 19.952151004324193 bad\n"
     ]
    }
   ],
   "source": [
    "#get word counts\n",
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, stop_words='english', sublinear_tf=True)\n",
    "train_features_stem = vectorizer.fit_transform([' '.join(review) for review in train_stem])\n",
    "test_features_stem = vectorizer.transform([' '.join(review) for review in test_stem])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features_stem, train_labels_stem)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features_stem)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels_stem, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 83.5%\n",
      "\n",
      "Important words in positive reviews\n",
      "1 845.0 of the\n",
      "1 786.0 in the\n",
      "1 684.0 the film\n",
      "1 657.0 to the\n",
      "1 635.0 and the\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 828.0 of the\n",
      "0 796.0 in the\n",
      "0 643.0 the film\n",
      "0 621.0 to be\n",
      "0 578.0 to the\n"
     ]
    }
   ],
   "source": [
    "#get word counts\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(2,2))\n",
    "train_features_stem = vectorizer.fit_transform([' '.join(review) for review in train_stem])\n",
    "test_features_stem = vectorizer.transform([' '.join(review) for review in test_stem])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features_stem, train_labels_stem)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features_stem)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels_stem, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, it seems like it is not worth to use the Porter stemmer. With the stemmer, the runtime is longer and the accuracy of the model does not change substantially (actually, it often decreases). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2312\n",
      "3324\n"
     ]
    }
   ],
   "source": [
    "NRC = pd.read_csv('NRC_Emotion.txt', sep = '\\t', names = ['TargetWord', 'AffectCategory', 'AssociationFlag'])\n",
    "#only two categories are relevant: positive and negative\n",
    "NRC = NRC[NRC.AffectCategory.isin(['positive', 'negative'])]\n",
    "#only words that are associated with affect category are relevant\n",
    "NRC = NRC[NRC.AssociationFlag==1]\n",
    "positive_words = list(NRC.loc[NRC.AffectCategory == 'positive', 'TargetWord'])\n",
    "negative_words = list(NRC.loc[NRC.AffectCategory == 'negative', 'TargetWord'])\n",
    "print(len(positive_words))\n",
    "print(len(negative_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get ratios in train and test data\n",
    "train_ratios = [pos_neg_word_ratio(review, positive_words, negative_words) for review in train]\n",
    "test_ratios = [pos_neg_word_ratio(review, positive_words, negative_words) for review in test]\n",
    "\n",
    "#train Naive Bayes classifier\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(np.array(train_ratios).reshape(-1, 1), train_labels)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(np.array(test_ratios).reshape(-1, 1))\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is by far the worst performing model. One possible explanation is to be found in the size of the dictionary. We only have 2312 positive words and 3324 negative words; moreover there is no guarantee that these words will cover the terms normally used to review a movie. This could be one of the factors contributing to the poor performance of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will focus on Model 1 only, however most of the considerations presented here can be applied to the other models to try and improve their performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_SW = list(stopwords.words('english'))\n",
    "updated_SW\n",
    "\n",
    "updated_SW.append(\"film\")\n",
    "updated_SW.append(\"movie\")\n",
    "updated_SW.append(\"character\")\n",
    "updated_SW.append(\"characters\")\n",
    "updated_SW.append(\"plot\")\n",
    "updated_SW.append(\"story\")\n",
    "updated_SW.append(\"time\")\n",
    "updated_SW.append(\"like\")\n",
    "updated_SW.append(\"would\")\n",
    "updated_SW.append(\"could\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word counts\n",
    "vectorizer = CountVectorizer(binary=True, stop_words=updated_SW)\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition suggests that removing stopwords would help the performance of the model. Surprisingly, this is not the case, as accuracy drops after removing stopwords. Marginal improvements happen when we start implementing a personalized list of stop words that focuses on words that often found in movie reviews and do not have implications on the sentiment of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word counts\n",
    "vectorizer = CountVectorizer(binary=True, max_df=0.75, min_df=5)\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "\n",
    "#train Naive Bayes classifier \n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "#get predictions\n",
    "predictions = nb_clf.predict(test_features)\n",
    "\n",
    "#evaluate classifier\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print('Accuracy is {}%\\n'.format(accuracy*100))\n",
    "\n",
    "# print most informative words\n",
    "show_most_informative_features(vectorizer, nb_clf, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the performance of the model by tuning the `max_df` and `min_df` parameters.   \n",
    "`max_df = 0.75` allows to automatically detect and filter stop words based on intra corpus document frequency of terms. When building the vocabulary, terms that have a document frequency strictly higher than 75% are ignored.   \n",
    "`min_df = 5` allows to remove infrequent words. When building the vocabulary, terms that appear in less than 5 documents are ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization could also help improve the performance of this model, as the length of reviews varies drastically across different people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
